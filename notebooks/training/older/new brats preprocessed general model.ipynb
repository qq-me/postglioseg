{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, functools, statistics\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import dill, joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from glio.train2 import *\n",
    "from glio.train2.cbs_summary import Summary\n",
    "from glio.visualize import vis_imshow, vis_imshow_grid, Visualizer\n",
    "from glio.jupyter_tools import show_slices, show_slices_arr, clean_mem\n",
    "from glio.torch_tools import area_around, one_hot_mask, summary, lr_finder, to_binary, count_parameters, replace_conv, replace_conv_transpose, replace_layers\n",
    "from glio.python_tools import type_str, CacheRepeatIterator, get_all_files\n",
    "from glio import nn as gnn\n",
    "from glio.nn import conv, convt, linear, seq, block\n",
    "from glio.data import DSToTarget\n",
    "from glio.helpers import cnn_output_size, tcnn_output_size\n",
    "from glio.loaders import nifti\n",
    "from glio.transforms import fToChannels, fToChannelsFirst,fToChannelsLast, z_normalize, norm_to01\n",
    "\n",
    "from monai.losses.dice import DiceLoss, DiceFocalLoss, GeneralizedDiceFocalLoss\n",
    "from monai.losses.ssim_loss import SSIMLoss\n",
    "from monai.metrics import MeanIoU, SurfaceDiceMetric, DiceHelper, compute_iou, compute_dice # type:ignore\n",
    "\n",
    "from schedulefree import AdamWScheduleFree\n",
    "from came_pytorch import CAME\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = \"BRaTS2024 histnorm 2D\"\n",
    "PRELOAD = 1.0\n",
    "AREA_AROUND = 96,96\n",
    "DEPTH_AROUND = 1\n",
    "\n",
    "def loader(d: dict[str, Any], wh_around = AREA_AROUND, d_around = DEPTH_AROUND):\n",
    "    # load center\n",
    "    center = d[\"center\"]\n",
    "    # load seg\n",
    "    seg = d[\"seg\"]\n",
    "    if isinstance(seg, str): seg = area_around(torch.from_numpy(np.asanyarray(d[\"obs\"][seg][d[\"slice\"]])), center, wh_around)\n",
    "    if d_around == 0:\n",
    "        # return 2D array around center and 2D seg array around center\n",
    "        return (\n",
    "            area_around(torch.from_numpy(np.asanyarray(d[\"obs\"][d[\"dim\"]][d[\"prep\"]][d[\"mod\"]][d[\"slice\"]])), center, wh_around),\n",
    "            one_hot_mask(seg, 4)\n",
    "        )\n",
    "    else:\n",
    "        # return (d_around*2+1, h, w) array around center and 2D seg array around center\n",
    "        return (\n",
    "            area_around(torch.from_numpy(np.asanyarray(d[\"obs\"][d[\"dim\"]][d[\"prep\"]][d[\"mod\"]][d[\"slice\"]-d_around:d[\"slice\"]+d_around+1])), center, AREA_AROUND),\n",
    "            one_hot_mask(seg, 4)\n",
    "        )\n",
    "\n",
    "def loader_multimod(d: dict[str, Any], wh_around = AREA_AROUND, d_around = DEPTH_AROUND):\n",
    "    # load center\n",
    "    center = d[\"center\"]\n",
    "    # load seg\n",
    "    seg = d[\"seg\"]\n",
    "    if isinstance(seg, str): seg = area_around(torch.from_numpy(np.asanyarray(d[\"obs\"][seg][d[\"slice\"]])), center, wh_around)\n",
    "    # get prep slice\n",
    "    if d[\"prep\"] is None: prepslice = slice(None)\n",
    "    else: prepslice = d[\"prep\"]\n",
    "    # get mod slice\n",
    "    if d[\"mod\"] is None: modslice = slice(None)\n",
    "    else: modslice = d[\"mod\"]\n",
    "    if d_around == 0:\n",
    "        # return 2D array around center and 2D seg array around center\n",
    "        return (\n",
    "            area_around(torch.from_numpy(np.asanyarray(d[\"obs\"][d[\"dim\"]][prepslice, modslice, d[\"slice\"]])).flatten(0, -3), center, wh_around)[:,:wh_around[0],:wh_around[1]],\n",
    "            one_hot_mask(seg, 4)\n",
    "        )\n",
    "    else:\n",
    "        # return (d_around*2+1, h, w) array around center and 2D seg array around center\n",
    "        return (\n",
    "            area_around(torch.from_numpy(np.asanyarray(d[\"obs\"][d[\"dim\"]][prepslice, modslice, d[\"slice\"]-d_around:d[\"slice\"]+d_around+1])).flatten(0, -3), center, AREA_AROUND)[:,:wh_around[0],:wh_around[1]],\n",
    "            one_hot_mask(seg, 4)\n",
    "        )\n",
    "\n",
    "def tfm_input(data):\n",
    "    return data[0]\n",
    "\n",
    "def tfm_target(data):\n",
    "    return data[1]\n",
    "\n",
    "def add_zipstore_to_ds(\n",
    "    ds:DSToTarget,\n",
    "    store:zarr.ZipStore,\n",
    "    loader=loader,\n",
    "    tfminit=None,\n",
    "    tfminp=tfm_input,\n",
    "    tfmtarg=tfm_target,\n",
    "    dims = (\"top\", \"side\", \"front\"),\n",
    "    prep=(\"hist\",\"nohist\"),\n",
    "    mods=(\"t1c\",\"t1n\",\"t2f\",\"t2w\"),\n",
    "    mergeprep = False,\n",
    "    mergemods = False,\n",
    "    slstart = 0,\n",
    "    slend = 0,\n",
    "    wh_around = AREA_AROUND,\n",
    "    loadseg=False,\n",
    "    ):\n",
    "    # convert hist/nohist to index\n",
    "    prep = [(0 if i.lower()==\"nohist\" else 1) for i in prep]\n",
    "    # convert modality to index\n",
    "    allmods = (\"t1c\",\"t1n\",\"t2f\",\"t2w\")\n",
    "    mods = [allmods.index(i.lower()) for i in mods]\n",
    "    # init group\n",
    "    arr = zarr.group(store)\n",
    "    # for all observations\n",
    "    for obs in arr.values():\n",
    "        # for top/front/side\n",
    "        for dim in dims:\n",
    "            # for hist/nohist\n",
    "            for p in [None] if mergeprep else prep:\n",
    "                # for t1c/t1n/t2f/t2w\n",
    "                for m in [None] if mergemods else mods:\n",
    "                    centers = obs[f\"{dim}_centers\"]\n",
    "                    # for all slices along first dim, saving the center\n",
    "                    for i,center in enumerate(centers):\n",
    "                        # check if slice is in range\n",
    "                        if slstart <= i < (len(centers) - slend):\n",
    "                            # save segmentation as is doesnt take up much RAM\n",
    "                            if loadseg: seg = area_around(torch.from_numpy(np.asanyarray(obs[f\"{dim}_seg\"][i])), center, wh_around).clone()\n",
    "                            else: seg = f\"{dim}_seg\"\n",
    "                            # add sample as dict\n",
    "                            ds.add_sample((dict(obs=obs, dim=dim, prep=p, mod=m, slice=i, center=torch.from_numpy(np.asanyarray(center)), seg=seg)),\n",
    "                                        loader, tfminit, tfminp, tfmtarg)\n",
    "\n",
    "def plot_preds(learner:Learner, batch, softmax = True, unsqueeze = True):\n",
    "    batch = list(batch)\n",
    "    if unsqueeze:\n",
    "        batch[0] = batch[0].unsqueeze(0)\n",
    "        batch[1] = batch[1].unsqueeze(0)\n",
    "    preds = learner.inference(batch[0])\n",
    "    v = Visualizer()\n",
    "    v.imshow_grid(batch[0][0], mode=\"bhw\", label=\"вход\")\n",
    "    v.imshow_grid(batch[1][0], mode=\"bhw\", label = \"реальная карта\")\n",
    "    if softmax:\n",
    "        output = torch.stack([preds[0],preds[0],preds[0]], dim=1)\n",
    "        output[:,0] *=  F.softmax(preds[0],0)\n",
    "        v.imshow_grid(output, mode=\"bchw\", label=\"сырой выход\")\n",
    "        v.imshow_grid(to_binary(F.softmax(preds[0], 0)), mode=\"bhw\", label=\"предсказанная карта\")\n",
    "    else:\n",
    "        v.imshow_grid(preds[0], mode=\"bchw\", label=\"сырой выход\")\n",
    "        v.imshow_grid(to_binary(preds[0], 0), mode=\"bhw\", label=\"предсказанная карта\")\n",
    "    v.show(figsize=(24, 24), nrows=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = [zarr.ZipStore(i, mode=\"r\") for i in get_all_files(r\"E:\\dataset\\BRaTS2024 2D full norm\", extensions=\"zip\")]\n",
    "len(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding\n"
     ]
    }
   ],
   "source": [
    "dstrain = DSToTarget()\n",
    "dstest = DSToTarget()\n",
    "print('adding')\n",
    "for i,store in enumerate(stores[:22]):\n",
    "    print(i, end='\\r')\n",
    "    add_zipstore_to_ds(dstrain, store, slstart=1, slend=-1, loader=loader_multimod, mergemods=True, mergeprep=True)\n",
    "for i,store in enumerate(stores[22:]):\n",
    "    print(i, end='\\r')\n",
    "    add_zipstore_to_ds(dstest, store, slstart=1, slend=-1, loader=loader_multimod, mergemods=True, mergeprep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstest.set_loader(loader_multimod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path                                         module                                       input size               output size              params    buffers   \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/conv_inittorch.nn.modules.conv.Conv2d                 (8, 24, 96, 96)          (8, 16, 96, 96)          3456      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 16, 96, 96)          (8, 16, 96, 96)          32        33        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 16, 96, 96)          (8, 16, 96, 96)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 16, 96, 96)          (8, 16, 96, 96)          2304      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 16, 96, 96)          (8, 16, 96, 96)          32        33        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 16, 96, 96)          (8, 16, 96, 96)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 16, 96, 96)          (8, 16, 96, 96)          2304      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/0/downsampletorch.nn.modules.conv.Conv2d                 (8, 16, 96, 96)          (8, 32, 48, 48)          4608      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 32, 48, 48)          (8, 32, 48, 48)          64        65        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 32, 48, 48)          (8, 32, 48, 48)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 32, 48, 48)          9216      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 32, 48, 48)          (8, 32, 48, 48)          64        65        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 32, 48, 48)          (8, 32, 48, 48)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 32, 48, 48)          9216      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/1/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 32, 48, 48)          (8, 32, 48, 48)          64        65        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/1/act1torch.nn.modules.activation.ReLU             (8, 32, 48, 48)          (8, 32, 48, 48)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/1/conv1torch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 32, 48, 48)          9216      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/1/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 32, 48, 48)          (8, 32, 48, 48)          64        65        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/1/act2torch.nn.modules.activation.ReLU             (8, 32, 48, 48)          (8, 32, 48, 48)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/blocks/1/conv2torch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 32, 48, 48)          9216      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/1/downsampletorch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 64, 24, 24)          18432     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 64, 24, 24)          (8, 64, 24, 24)          128       129       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 64, 24, 24)          (8, 64, 24, 24)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 64, 24, 24)          36864     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 64, 24, 24)          (8, 64, 24, 24)          128       129       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 64, 24, 24)          (8, 64, 24, 24)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 64, 24, 24)          36864     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/1/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 64, 24, 24)          (8, 64, 24, 24)          128       129       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/1/act1torch.nn.modules.activation.ReLU             (8, 64, 24, 24)          (8, 64, 24, 24)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/1/conv1torch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 64, 24, 24)          36864     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/1/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 64, 24, 24)          (8, 64, 24, 24)          128       129       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/1/act2torch.nn.modules.activation.ReLU             (8, 64, 24, 24)          (8, 64, 24, 24)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/blocks/1/conv2torch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 64, 24, 24)          36864     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/2/downsampletorch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 128, 12, 12)         73728     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/1/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/1/act1torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/1/conv1torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/1/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/1/act2torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/1/conv2torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/2/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/2/act1torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/2/conv1torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/2/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/2/act2torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/2/conv2torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/3/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/3/act1torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/3/conv1torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/3/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 128, 12, 12)         (8, 128, 12, 12)         256       257       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/3/act2torch.nn.modules.activation.ReLU             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/blocks/3/conv2torch.nn.modules.conv.Conv2d                 (8, 128, 12, 12)         (8, 128, 12, 12)         147456    0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/encoder/layers/3/downsampletorch.nn.modules.linear.Identity             (8, 128, 12, 12)         (8, 128, 12, 12)         0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/upsample/deconvtorch.nn.modules.conv.ConvTranspose2d        (8, 128, 12, 12)         (8, 64, 24, 24)          73728     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 64, 24, 24)          (8, 64, 24, 24)          128       129       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 64, 24, 24)          (8, 64, 24, 24)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 64, 24, 24)          36864     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 64, 24, 24)          (8, 64, 24, 24)          128       129       \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 64, 24, 24)          (8, 64, 24, 24)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/0/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 64, 24, 24)          (8, 64, 24, 24)          36864     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/upsample/deconvtorch.nn.modules.conv.ConvTranspose2d        (8, 64, 24, 24)          (8, 32, 48, 48)          18432     0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 32, 48, 48)          (8, 32, 48, 48)          64        65        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 32, 48, 48)          (8, 32, 48, 48)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 32, 48, 48)          9216      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 32, 48, 48)          (8, 32, 48, 48)          64        65        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 32, 48, 48)          (8, 32, 48, 48)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/1/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 32, 48, 48)          (8, 32, 48, 48)          9216      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/upsample/deconvtorch.nn.modules.conv.ConvTranspose2d        (8, 32, 48, 48)          (8, 16, 96, 96)          4608      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/blocks/0/norm1torch.nn.modules.batchnorm.BatchNorm2d       (8, 16, 96, 96)          (8, 16, 96, 96)          32        33        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/blocks/0/act1torch.nn.modules.activation.ReLU             (8, 16, 96, 96)          (8, 16, 96, 96)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/blocks/0/conv1torch.nn.modules.conv.Conv2d                 (8, 16, 96, 96)          (8, 16, 96, 96)          2304      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/blocks/0/norm2torch.nn.modules.batchnorm.BatchNorm2d       (8, 16, 96, 96)          (8, 16, 96, 96)          32        33        \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/blocks/0/act2torch.nn.modules.activation.ReLU             (8, 16, 96, 96)          (8, 16, 96, 96)          0         0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/blocks/0/conv2torch.nn.modules.conv.Conv2d                 (8, 16, 96, 96)          (8, 16, 96, 96)          2304      0         \n",
      "monai.networks.nets.segresnet_ds.SegResNetDS/up_layers/2/headtorch.nn.modules.conv.Conv2d                 (8, 16, 96, 96)          (8, 4, 96, 96)           68        0         \n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SegResNetDS # type:ignore\n",
    "\n",
    "summary(SegResNetDS(2, in_channels=24, out_channels=4, init_filters=16), (8, 24, 96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [24, 96, 96] at entry 0 and [16, 96, 96] at entry 41",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m SCHED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     15\u001b[0m clean_mem()\n\u001b[1;32m---> 16\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSUV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdltrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m LEARNER_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLOSS_FN\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mhasattr\u001b[39m(LOSS_FN,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mtype_str(LOSS_FN)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m opt=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPT\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCHED\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m learner \u001b[38;5;241m=\u001b[39m Learner(MODEL, LEARNER_NAME,\n\u001b[0;32m     20\u001b[0m                   cbs \u001b[38;5;241m=\u001b[39m (Metric_Loss(), \u001b[38;5;66;03m# Log_GradHistorgram(16), Log_SignalHistorgram(16),\u001b[39;00m\n\u001b[0;32m     21\u001b[0m                          Log_Time(), Save_Best(TITLE), Save_Last(TITLE), Log_LR(), Summary(), Accelerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39mOPT,\n\u001b[0;32m     30\u001b[0m                   scheduler\u001b[38;5;241m=\u001b[39mSCHED,)\n",
      "File \u001b[1;32mF:\\Stuff\\Programming\\AI\\glio_diff\\glio\\nn\\init\\LSUV.py:67\u001b[0m, in \u001b[0;36mLSUV\u001b[1;34m(model, dl, tolerance, max_iter, max_batches, log, device)\u001b[0m\n\u001b[0;32m     65\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[0;32m     68\u001b[0m         model(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;66;03m# print(list(LSUVer.modules_iterations.values()), i, min(list(LSUVer.modules_iterations.values())), max_iter)\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [24, 96, 96] at entry 0 and [16, 96, 96] at entry 41"
     ]
    }
   ],
   "source": [
    "MODEL = SegResNetDS(2, in_channels=24, out_channels=4, init_filters=24)\n",
    "NAME = f\"{MODEL.__class__.__name__}\"\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# dl_train = DataLoader(ds_train, BATCH_SIZE)\n",
    "dltrain = DataLoader(dstrain, BATCH_SIZE, shuffle=True)\n",
    "dltest = DataLoader(dstest, BATCH_SIZE)\n",
    "\n",
    "OPT = AdamWScheduleFree(MODEL.parameters(), lr=LR, eps=1e-6)\n",
    "LOSS_FN = DiceFocalLoss(softmax=True)\n",
    "SCHED = None\n",
    "\n",
    "clean_mem()\n",
    "MODEL = gnn.LSUV(MODEL, dltrain, max_iter=10)\n",
    "\n",
    "LEARNER_NAME = f\"{NAME} lr={LR} bs={BATCH_SIZE} loss = {LOSS_FN.__name__ if hasattr(LOSS_FN, '__name__') else type_str(LOSS_FN)} opt={OPT.__class__.__name__} sch={SCHED.__class__.__name__}\"\n",
    "learner = Learner(MODEL, LEARNER_NAME,\n",
    "                  cbs = (Metric_Loss(), # Log_GradHistorgram(16), Log_SignalHistorgram(16),\n",
    "                         Log_Time(), Save_Best(TITLE), Save_Last(TITLE), Log_LR(), Summary(), Accelerate(\"no\"),\n",
    "                         Metric_PredsTargetsFn(DiceLoss(softmax=True), step=4, name=\"dice loss\"), Metric_Accuracy(True, True, True),\n",
    "                         Metric_PredsTargetsFn(lambda x,y:compute_iou(to_binary(F.softmax(x[:,1:], 1)), y[:,1:]).nanmean(), step=8, name = \"iou\"),\n",
    "                         Metric_PredsTargetsFn(lambda x,y:compute_dice(to_binary(F.softmax(x[:,1:], 1)), y[:,1:]).nanmean(), step=8, name=\"dice coeff\"),\n",
    "                         FastProgressBar(plot=True, step_batch=16, metrics=\n",
    "                                         [\"train loss\", \"test loss\", \"train dice coeff\", \"test dice coeff\", \"train iou\", \"test iou\"]),\n",
    "                         ),\n",
    "                  loss_fn=LOSS_FN,\n",
    "                  optimizer=OPT,\n",
    "                  scheduler=SCHED,)\n",
    "learner.fit(N_EPOCHS, dltrain, dltest)\n",
    "\n",
    "plt.show()\n",
    "plot_preds(learner, dstrain[3], softmax=True)\n",
    "plot_preds(learner, dstrain[4], softmax=True)\n",
    "plot_preds(learner, dstrain[6], softmax=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
